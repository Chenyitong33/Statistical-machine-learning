{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Digit Recognition\n",
    "\n",
    "## Statistical Machine Learning (COMP90051), Semester 2, 2017\n",
    "\n",
    "*Copyright the University of Melbourne, 2017*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitted by:  Yitong Chen\n",
    "### Student number: 879326\n",
    "### Kaggle-in-class username: *YitongChen*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, you will be applying machine learning for recognising digits from real world images. The project worksheet is a combination of text, pre-implemented code and placeholders where we expect you to add your code and answers. You code should produce desired result within a reasonable amount of time. Please follow the instructions carefully, **write your code and give answers only where specifically asked**. In addition to worksheet completion, you are also expected to participate **live competition with other students in the class**. The competition will be run using an on-line platform called Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Marking:** You can get up to 33 marks for Project 2. The sum of marks for Project 1 and Project 2 is then capped to 50 marks\n",
    "\n",
    "**Due date:** Wednesday 11/Oct/17, 11:59pm AEST (LMS components); and Kaggle competition closes Monday 09/Oct/17, 11:59pm AEST.\n",
    "\n",
    "**Late submissions** will incur a 10% penalty per calendar day\n",
    "\n",
    "** Submission materials**\n",
    " - **Worksheet**: Fill in your code and answers within this IPython Notebook worksheet.\n",
    " - **Competition**: Follow the instructions provided in the corresponding section of this worksheet. Your competition submissions should be made via Kaggle website.\n",
    " - **Report**: The report about your competition entry should be submitted to the LMS as a PDF file (see format requirements in `2.2`).\n",
    " - **Code**: The source code behind your competition entry.\n",
    "The **Worksheet**, **Report** and **Code** should be bundled into a `.zip` file (not 7z, rar, tar, etc) and submitted in the LMS. Marks will be deducted for submitting files in other formats, or we may elect not to mark them at all.\n",
    "\n",
    "**Academic Misconduct:** Your submission should contain only your own work and ideas. Where asked to write code, you cannot re-use someone else's code, and should write your own implementation. We will be checking submissions for originality and will invoke the University’s <a href=\"http://academichonesty.unimelb.edu.au/policy.html\">Academic Misconduct policy</a> where inappropriate levels of collusion or plagiarism are deemed to have taken place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of Contents**\n",
    "\n",
    "1. Handwritten Digit Recognition **(16 marks)**\n",
    "  1. Linear Approach\n",
    "  2. Basis Expansion\n",
    "  3. Kernel Perceptron\n",
    "  4. Dimensionality Reduction\n",
    "  \n",
    "2. Kaggle Competition **(17 marks)**\n",
    "  1. Making Submissions\n",
    "  2. Method Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Handwritten Digit Recognition\n",
    "Handwritten digit recognition can be framed as a classification task: given a bitmap image as input, predict the digit type (0, 1, ..., 9). The pixel values in each position of the image form our features, and the digit type is the class. We are going to use a dataset where the digits are represented as *28 x 28* bitmap images. Each pixel value ranges between 0 and 1, and represents the monochrome ink intensity at that position. Each image matrix has been flattened into one long feature vector, by concatenating each row of pixels.\n",
    "\n",
    "In this part of the project, we will only use images of two digits, namely \"7\" and \"9\". As such, we will be working on a binary classification problem. *Throughout this first section, our solution is going to be based on the perceptron classifier.*\n",
    "\n",
    "Start by setting up working environment, and loading the dataset. *Do not override variable `digits`, as this will be used throughout this section.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "\n",
    "digits = np.loadtxt('digits_7_vs_9.csv', delimiter=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take some time to explore the dataset. Note that each image of \"7\" is labeled as -1, and each image of \"9\" is labeled as +1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract a stack of 28x28 bitmaps\n",
    "X = digits[:, 0:784]\n",
    "# extract labels for each bitmap\n",
    "y = digits[:, 784:785]\n",
    "# display a single bitmap and print its label\n",
    "bitmap_index = 0\n",
    "plt.imshow(X[bitmap_index,:].reshape(28, 28), interpolation=None)\n",
    "print(y[bitmap_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also display several bitmaps at once using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gallery(array, ncols):\n",
    "    nindex, height, width = array.shape\n",
    "    nrows = nindex//ncols\n",
    "    result = (array.reshape((nrows, ncols, height, width))\n",
    "              .swapaxes(1,2)\n",
    "              .reshape((height*nrows, width*ncols)))\n",
    "    return result\n",
    "\n",
    "ncols = 10\n",
    "result = gallery(X.reshape((300, 28, 28))[:ncols**2], ncols)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(result, interpolation=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Linear Approach\n",
    "We are going to use perceptron for our binary classification task. Recall that perceptron is a linear method. Also, for this first step, we will not apply non-linear transformations to the data.\n",
    "\n",
    "Implement and fit a perceptron to the data above. You may use the implementation from *sklearn*, or implementation from one of our workshops. Report the error of the fit as the proportion of misclassified examples.\n",
    "\n",
    "<br />\n",
    "\n",
    "<font color='red'>**Write your code in the cell below ...**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## your code here\n",
    "# To do this, simply concatenate a column of 1s to the data matrix.\n",
    "Phi = np.column_stack([np.ones(X.shape[0]), X])\n",
    "# Prediction function\n",
    "def perc_pred(phi, w):\n",
    "    return np.sign(np.sign(np.dot(phi, w)) + 0.5)\n",
    "\n",
    "# Training algorithm\n",
    "def train(data, target, epochs, w , eta= 1.):\n",
    "    for e in range(epochs):\n",
    "        for i in range(data.shape[0]):\n",
    "            yhat = perc_pred(data[i,:], w)\n",
    "            if yhat != target[i]:\n",
    "                w += eta * target[i] * data[i]\n",
    "    return w\n",
    "\n",
    "# Run the training algorithm for 100 epochs to learn the weights\n",
    "w = np.zeros(Phi.shape[1])\n",
    "w = train(Phi, y, 100, w)\n",
    "print(w.shape)\n",
    "\n",
    "Error = np.sum(perc_pred(Phi, w).reshape(y.shape[0],1) != y) / float(y.shape[0])\n",
    "print(Error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the advantages of a linear approach is the ability to interpret results. To this end, plot the parameters learned above. Exclude the bias term if you were using it, set $w$ to be the learned perceptron weights, and run the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(w.reshape(1,785))\n",
    "w = np.delete(w.reshape(1,785),0,1)\n",
    "plt.imshow(w.reshape(28,28), interpolation=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a few sentences, describe what you see, referencing which features are most important for making classification. Report any evidence of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**Write your answer here ...**</font> (as a *markdown* cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The importance of a feature is captured by computing how much the learned model depends on, as the epoch here.\n",
    "The result is blurry, and the error proportion of misclassified examples is always 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and heldout validation partitions by holding out a random 25% sample of the data. Evaluate the error over the course of a training run, and plot the training and validation error rates as a function of the number of passes over the training dataset.\n",
    "\n",
    "<br />\n",
    "<font color='red'>**Write your code in the cell below ...**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## your code here\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "phi_train, phi_test, y_train, y_test = train_test_split(Phi, y, \n",
    "                                                    test_size=0.25, \n",
    "                                                    random_state=0)\n",
    "\n",
    "w_hat = np.zeros(Phi.shape[1])\n",
    "T = 60\n",
    "train_error = np.zeros(T)\n",
    "heldout_error = np.zeros(T)\n",
    "for ep in range(T):\n",
    "    # here we use a learning rate, which decays with each epoch\n",
    "    lr = 1./(1+ep)\n",
    "    w_hat = train(Phi, y, 1, w_hat, eta = lr )\n",
    "    #print(w_hat)\n",
    "    train_error[ep] = np.sum(perc_pred(phi_train, w_hat).reshape(y_train.shape[0],1) != y_train) / np.float(y_train.shape[0])\n",
    "    heldout_error[ep] = np.sum(perc_pred(phi_test, w_hat).reshape(y_test.shape[0],1) != y_test) / np.float(y_test.shape[0])\n",
    "\n",
    "plot(train_error, label = 'Train Error')\n",
    "plot(heldout_error, label = 'Held-out Error')\n",
    "plt.legend()\n",
    "xlabel('Epochs')\n",
    "ylabel('Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a few sentences, describe the shape of the curves, and compare the two. Now consider if we were to stop training early, can you choose a point such that you get the best classification performance? Justify your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**Write your answer here ...**</font> (as a *markdown* cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape was at a very high point at the beginning, and then went down sharply, but it increased a little at around 7 epoches, finally decreased to an even low level at around 9, so it is not good to stop training early. We may choose the point as 10, which is a reasonable low point for both train and holdout error rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have tried a simple approach, we are going to implement several non-linear approaches to our task. Note that we are still going to use a linear method (the perceptron), but combine this with a non-linear data transformation. We start with basis expansion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Basis Expansion\n",
    "Apply Radial Basis Function (RBF)-based transformation to the data, and fit a perceptron model. Recall that the RBF basis is defined as\n",
    "\n",
    "$$\\varphi_l(\\mathbf{x}) =  \\exp\\left(-\\frac{||\\mathbf{x} - \\mathbf{z}_l||^2}{\\sigma^2}\\right)$$\n",
    "\n",
    "where $\\mathbf{z}_l$ is centre of the $l^{th}$ RBF. We'll use $L$ RBFs, such that $\\varphi(\\mathbf{x})$ is a vector with $L$ elements. The spread parameter $\\sigma$ will be the same for each RBF.\n",
    "\n",
    "*Hint: You will need to choose the values for $\\mathbf{z}_l$ and $\\sigma$. If the input data were 1D, the centres $\\mathbf{z}_l$ could be uniformly spaced on a line. However, here we have 784-dimensional input. For this reason you might want to use some of the training points as centres, e.g., $L$ randomly chosen \"2\"s and \"7\"s.*\n",
    "\n",
    "<br />\n",
    "\n",
    "<font color='red'>**Write your code in the cell below ...**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## your code here\n",
    "#Each RBF basis function should take x (a 784 dimensional vector), and return a scalar. \n",
    "#Each RBF will be parameterised by a centre (784 dimensional vector) and a length scale (scalar). \n",
    "#The return scalar is computed based on the distance between x and the centre, as shown in the mathematical formulation.  \n",
    "#Consequently phi(x) should return a vector of size L, and accordingly Phi(X) for the whole dataset will be a matrix N x L. \n",
    "\n",
    "# Input:\n",
    "# x - is a column vector of input values\n",
    "# z - is a scalar that controls location\n",
    "# s - is a scalar that controls spread\n",
    "#\n",
    "# Output:\n",
    "# v - contains the values of RBF evaluated for each element x\n",
    "#     v has the same dimensionality as x\n",
    "def radial_basis_function(x, z, s):\n",
    "    # ensure that t is a column vector\n",
    "    '''\n",
    "    x = np.array(x)\n",
    "    if x.size == 1:\n",
    "        x.shape = (1,1)\n",
    "    else:\n",
    "        x_length = x.shape[0]\n",
    "        x.shape = (x_length, 1)\n",
    "    '''\n",
    "    # compute RBF value\n",
    "    r = np.linalg.norm(x - z)\n",
    "    v = np.exp(-r**2/(s**2))\n",
    "    return v\n",
    "\n",
    "# Input:\n",
    "# x - is an Nx784 column vector\n",
    "# z - is an Lx784 column vector with locations for each of M RBFs\n",
    "# s - is a scalar that controls spread, shared between all RBFs\n",
    "#\n",
    "# Output:\n",
    "# Phi - is an NxL matrix, such that Phi(i,j) is the \n",
    "#       RBF transformation of x(i) using location z(j) and scale s\n",
    "def expand_to_RBF(x, z, s):\n",
    "    #... your code here ...\n",
    "    #... in your code use \"radial_basis_function\" from above ...\n",
    "    L = z.shape[0]\n",
    "    N = x.shape[0]\n",
    "    Phi = np.zeros((N, L))\n",
    "    for i in range(N):\n",
    "        for j in range(L):\n",
    "        \n",
    "            #y_rbf = radial_basis_function(x_rbf, z[i], sigma)\n",
    "            v = radial_basis_function(x[i], z[j], sigma)\n",
    "            Phi[i,j] = v\n",
    "            \n",
    "    \n",
    "    return Phi\n",
    "\n",
    "# set L to 60 and sigma to 0.01\n",
    "l = 60\n",
    "z = X[np.random.choice(X.shape[0], l, replace=False), :]\n",
    "sigma = 0.01 # same scale for each RBF\n",
    "\n",
    "# use \"expand_to_RBF\" function from above\n",
    "x = expand_to_RBF(X, z, sigma)\n",
    "print(x.shape)\n",
    "x_dummy = np.ones(X.shape[0])\n",
    "X_expand = np.column_stack((x_dummy, x))\n",
    "print(X_expand.shape)\n",
    "\n",
    "# Run the training algorithm for 100 epochs to learn the weights\n",
    "w = np.zeros(X_expand.shape[1])\n",
    "w = train(X_expand, y, 100, w)\n",
    "print(w.shape)\n",
    "\n",
    "Error = np.sum(perc_pred(X_expand, w).reshape(y.shape[0],1) != y) / float(y.shape[0])\n",
    "print(Error)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute the validation error for your RBF-perceptron and use this to choose good values of $L$ and $\\sigma$. Show a plot of the effect of changing each of these parameters, and justify your parameter choice.\n",
    "\n",
    "<br />\n",
    "\n",
    "<font color='red'>**Write your code in the cell below ...**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## your code here\n",
    "## when sigma is always 0.01\n",
    "sigma = 0.01 # same scale for each RBF\n",
    "train_error = np.zeros(300)\n",
    "heldout_error = np.zeros(300)\n",
    "for L in range(300):\n",
    "    z = X[np.random.choice(X.shape[0], L, replace=False), :]\n",
    "    x = expand_to_RBF(X, z, sigma)\n",
    "    #print(x.shape)\n",
    "    x_dummy = np.ones(X.shape[0])\n",
    "    X_expand = np.column_stack((x_dummy, x))\n",
    "    w_hat = np.zeros(X_expand.shape[1])\n",
    "    w_hat = train(X_expand, y, 100, w_hat)\n",
    "    \n",
    "    phi_train, phi_test, y_train, y_test = train_test_split(X_expand, y, \n",
    "                                                    test_size=0.25, \n",
    "                                                    random_state=0)\n",
    "    #print(w_hat)\n",
    "    train_error[L] = np.sum(perc_pred(phi_train, w_hat).reshape(y_train.shape[0],1) != y_train) / np.float(y_train.shape[0])\n",
    "    heldout_error[L] = np.sum(perc_pred(phi_test, w_hat).reshape(y_test.shape[0],1) != y_test) / np.float(y_test.shape[0])\n",
    "plt.title('Errors for sigma = 0.01')\n",
    "plot(train_error, label = 'Train Error')\n",
    "plot(heldout_error, label = 'Held-out Error')\n",
    "plt.legend()\n",
    "xlabel('number of L')\n",
    "ylabel('Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# when L is always 300\n",
    "l = 300\n",
    "train_error_list = []\n",
    "heldout_error_list = []\n",
    "sigmas = [1e-10, 1e-6, 1e-4, 1e-2, 1, 100]\n",
    "for n, s in enumerate(sigmas):\n",
    "    z = X[np.random.choice(X.shape[0], l, replace=False), :]\n",
    "    x = expand_to_RBF(X, z, s)\n",
    "    #print(x.shape)\n",
    "    x_dummy = np.ones(X.shape[0])\n",
    "    X_expand = np.column_stack((x_dummy, x))\n",
    "    w_hat = np.zeros(X_expand.shape[1])\n",
    "    w_hat = train(X_expand, y, 100, w_hat)\n",
    "    \n",
    "    phi_train, phi_test, y_train, y_test = train_test_split(X_expand, y, \n",
    "                                                    test_size=0.25, \n",
    "                                                    random_state=0)\n",
    "    #print(w_hat)\n",
    "    train_error_list.append(np.sum(perc_pred(phi_train, w_hat).reshape(y_train.shape[0],1) != y_train) / np.float(y_train.shape[0]))\n",
    "    heldout_error_list.append(np.sum(perc_pred(phi_test, w_hat).reshape(y_test.shape[0],1) != y_test) / np.float(y_test.shape[0]))\n",
    "    \n",
    "plt.title('Errors for L = 300')\n",
    "plt.plot(sigmas[:6], np.asarray(train_error_list))\n",
    "plt.plot(sigmas[:6], np.asarray(heldout_error_list))\n",
    "plt.xlim((min(sigmas), max(sigmas)))\n",
    "plt.xscale('log')\n",
    "plt.ylim(0, 1)\n",
    "#plot(heldout_error, label = 'Held-out Error')\n",
    "xlabel('sigma')\n",
    "ylabel('Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**Write your justfication here ...**</font> (as a *markdown* cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the first plot, we can just set sigma to 0.01 at first, and found that the error rate went down when L became close to 300, thus we set the L to 300 when testing for sigma, as in the second plot. \n",
    "From the second plot, it seems that the sigma has no effect on the performance.\n",
    "As the result, we will just choose L as 300 with sigma 0.01."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Kernel Perceptron\n",
    "Next, instead of directly computing a feature space transformation, we are going to use the kernel trick. Specifically, we are going to use the kernelised version of perceptron in combination with a few different kernels.\n",
    "\n",
    "*In this section, you cannot use any libraries other than `numpy` and `matplotlib`.*\n",
    "\n",
    "First, implement linear, polynomial and RBF kernels. The linear kernel is simply a dot product of its inputs, i.e., there is no feature space transformation. Polynomial and RBF kernels should be implemented as defined in the lecture slides.\n",
    "\n",
    "<br />\n",
    "\n",
    "<font color='red'>**Write your code in the cell below ...**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input:\n",
    "# u,v - column vectors of the same dimensionality\n",
    "#\n",
    "# Output:\n",
    "# v - a scalar\n",
    "def linear_kernel(u, v):\n",
    "    ## your code here\n",
    "    z = np.dot(u.T, v)\n",
    "    return z\n",
    "# Input:\n",
    "# u,v - column vectors of the same dimensionality\n",
    "# c,d - scalar parameters of the kernel as defined in lecture slides\n",
    "#\n",
    "# Output:\n",
    "# v - a scalar\n",
    "def polynomial_kernel(u, v, c=0, d=3):\n",
    "    ## your code here\n",
    "    z = (np.dot(u.T, v)+c)**d\n",
    "    return z   \n",
    "   \n",
    "\n",
    "# Input:\n",
    "# u,v - column vectors of the same dimensionality\n",
    "# gamma - scalar parameter of the kernel as defined in lecture slides\n",
    "#\n",
    "# Output:\n",
    "# v - a scalar\n",
    "def rbf_kernel(u, v, gamma=1):\n",
    "    ## your code here\n",
    "    r = np.linalg.norm(u - v)\n",
    "    v = np.exp(-gamma*(r**2))\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel perceptron was a \"green slides\" topic, and you will not be asked about this method in the exam. Here, you are only asked to implement a simple prediction function following the provided equation. In kernel perceptron, the prediction for instance $\\mathbf{x}$ is made based on the sign of\n",
    "\n",
    "$$w_0 + \\sum_{i=1}^{n}\\alpha_i y_i K(\\mathbf{x}_i, \\mathbf{x})$$\n",
    "\n",
    "Here $w_0$ is the bias term, $n$ is the number of training examples, $\\alpha_i$ are learned weights, $\\mathbf{x}_i$ and $y_i$ is the training dataset,and $K$ is the kernel.\n",
    "\n",
    "<br />\n",
    "\n",
    "<font color='red'>**Write your code in the cell below ...**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input:\n",
    "# x_test - (r x m) matrix with instances for which to predict labels\n",
    "# X - (n x m) matrix with training instances in rows\n",
    "# y - (n x 1) vector with labels\n",
    "# alpha - (n x 1) vector with learned weigths\n",
    "# bias - scalar bias term\n",
    "# kernel - a kernel function that follows the same prototype as each of the three kernels defined above\n",
    "#\n",
    "# Output:\n",
    "# y_pred - (r x 1) vector of predicted labels\n",
    "\n",
    "def kernel_ptron_predict(x_test, X, y, alpha, bias, kernel, c=0 ,d=3 ,gamma=1):\n",
    "    ## your code here\n",
    "    # test x_test is a matrix or vector\n",
    "    if x_test.size == 784:\n",
    "        R = 1\n",
    "    else:\n",
    "        R = x_test.shape[0]\n",
    "    #R = int(x_test.shape[0]/784)\n",
    "    #print(R)\n",
    "    x_test = x_test.reshape(R,784)\n",
    "    N = X.shape[0]\n",
    "    y_pred = np.zeros((R,1))\n",
    "    for i in range(R):\n",
    "        for j in range(N):\n",
    "            if kernel == linear_kernel:\n",
    "                y_pred[i] += alpha[j]*y[j]*kernel(x_test[i],X[j])\n",
    "            elif kernel == polynomial_kernel:\n",
    "                y_pred[i] += alpha[j]*y[j]*kernel(x_test[i],X[j],c,d)\n",
    "            else:\n",
    "                y_pred[i] += alpha[j]*y[j]*kernel(x_test[i],X[j],gamma)\n",
    "        y_pred[i] += bias\n",
    "        y_pred[i]= np.sign(y_pred[i])\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for kernel perceptron training is provided below. You can treat this function as a black box, but we encourage you to understand the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input:\n",
    "# X - (n x m) matrix with training instances in rows\n",
    "# y - (n x 1) vector with labels\n",
    "# kernel - a kernel function that follows the same prototype as each of the three kernels defined above\n",
    "# epochs - scalar, number of epochs\n",
    "#\n",
    "# Output:\n",
    "# alpha - (n x 1) vector with learned weigths\n",
    "# bias - scalar bias term\n",
    "def kernel_ptron_train(X, y, kernel, epochs=100):\n",
    "    n, m = X.shape\n",
    "    alpha = np.zeros(n)\n",
    "    bias = 0\n",
    "    updates = None\n",
    "    for epoch in range(epochs):\n",
    "        print('epoch =', epoch, ', updates =', updates)\n",
    "        updates = 0\n",
    "\n",
    "        schedule = list(range(n))\n",
    "        np.random.shuffle(schedule)\n",
    "        for i in schedule:\n",
    "            y_pred = kernel_ptron_predict(X[i], X, y, alpha, bias, kernel)\n",
    "            \n",
    "            if y_pred != y[i]:\n",
    "                alpha[i] += 1\n",
    "                bias += y[i]\n",
    "                updates += 1\n",
    "\n",
    "        if updates == 0:\n",
    "            break\n",
    "        \n",
    "    return alpha, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the above functions to train the perceptron. Use heldout validation, and compute the validation error for this method using each of the three kernels. Write a paragraph or two with analysis how the accuracy differs between the different kernels and choice of kernel parameters. Discuss the merits of a kernel approach versus direct basis expansion approach as was used in the previous section.\n",
    "\n",
    "<br />\n",
    "\n",
    "<font color='red'>**Write your code in the cell below ...**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phi_train, phi_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.25, \n",
    "                                                    random_state=0)\n",
    "# Linear kernel\n",
    "print('LINEAR')\n",
    "alpha, bias = kernel_ptron_train(phi_train, y_train, linear_kernel)\n",
    "# exclude bias term\n",
    "#bias = 0\n",
    "Error = np.sum(kernel_ptron_predict(phi_test, phi_train, y_train, alpha, bias, linear_kernel) != y_test) / float(y_test.shape[0])\n",
    "print('Error =',Error)\n",
    "#print(kernel_ptron_predict(phi_test, phi_train, y_train, alpha, bias, linear_kernel))\n",
    "\n",
    "# Polynomial kernel\n",
    "print('POLYNOMIAL')\n",
    "alpha, bias = kernel_ptron_train(phi_train, y_train, polynomial_kernel)\n",
    "# exclude bias term\n",
    "#bias = 0\n",
    "d_list = [0, 1, 10, 100]\n",
    "for n, s in enumerate(d_list):\n",
    "    Error = np.sum(kernel_ptron_predict(phi_test, phi_train, y_train, alpha, bias, polynomial_kernel, d = s) != y_test) / float(y_test.shape[0])\n",
    "    print('d:',s,'Error =',Error)\n",
    "\n",
    "c_list = [0, 1e-2, 1, 10, 100]\n",
    "for n, s in enumerate(c_list):\n",
    "    Error = np.sum(kernel_ptron_predict(phi_test, phi_train, y_train, alpha, bias, polynomial_kernel, c = s) != y_test) / float(y_test.shape[0])\n",
    "    print('c:',s,'Error =',Error)\n",
    "#print(kernel_ptron_predict(phi_test, phi_train, y_train, alpha, bias, polynomial_kernel))\n",
    "\n",
    "# RBF kernel\n",
    "print('RBF')\n",
    "alpha, bias = kernel_ptron_train(phi_train, y_train, rbf_kernel)\n",
    "# exclude bias term\n",
    "#bias = 0\n",
    "g_list = [ 1e-10, 1e-6, 1e-4, 1e-2, 1, 10, 100]\n",
    "for n, s in enumerate(g_list):\n",
    "    Error = np.sum(kernel_ptron_predict(phi_test, phi_train, y_train, alpha, bias, rbf_kernel, gamma = s) != y_test) / float(y_test.shape[0])\n",
    "    print('g:',s,'Error =',Error)\n",
    "\n",
    "#print(kernel_ptron_predict(phi_test, phi_train, y_train, alpha, bias, rbf_kernel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color='red'>**Provide your analysis here ...**</font> (as a *markdown* cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the accuracy of the polynomial and rbf kernel are slightly better than the linear method, but they requir the good choice of parameters.\n",
    "As for the polynomial kernel, it may have the best accuracy when c is 10 as well as d is 0-1.\n",
    "As for the rbf kernel, it could have the best performance when gamma is 1.\n",
    "The basis expansion relies on a large number of L, whereas the kernel approach is much easier to get a high accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Dimensionality Reduction\n",
    "Yet another approach to working with complex data is to use a non-linear dimensionality reduction. To see how this might work, first apply a couple of dimensionality reduction methods and inspect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import manifold\n",
    "# extract a stack of 28x28 bitmaps\n",
    "#X = digits[:, 0:784]\n",
    "# extract labels for each bitmap\n",
    "y = digits[:, 784:785]\n",
    "#print(y)\n",
    "\n",
    "X = digits[:, 0:784]\n",
    "y = np.squeeze(digits[:, 784:785])\n",
    "\n",
    "print(y)\n",
    "# n_components refers to the number of dimensions after mapping\n",
    "# n_neighbors is used for graph construction\n",
    "X_iso = manifold.Isomap(n_neighbors=30, n_components=2).fit_transform(X)\n",
    "\n",
    "\n",
    "# n_components refers to the number of dimensions after mapping\n",
    "embedder = manifold.SpectralEmbedding(n_components=2, random_state=0)\n",
    "X_se = embedder.fit_transform(X)\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.plot(X_iso[y==-1,0], X_iso[y==-1,1], \"bo\")\n",
    "ax1.plot(X_iso[y==1,0], X_iso[y==1,1], \"ro\")\n",
    "ax1.set_title('Isomap')\n",
    "ax2.plot(X_se[y==-1,0], X_se[y==-1,1], \"bo\")\n",
    "ax2.plot(X_se[y==1,0], X_se[y==1,1], \"ro\")\n",
    "ax2.set_title('spectral')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a few sentences, explain how a dimensionality reduction algorithm can be used for your binary classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**Write your answer here ...**</font> (as a *markdown* cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ﬁelds in dimension reduction, linear techniques that use a linear mapping to reduce the dimension, and nonlinear technique makes the assumption that the data available is embedded on a manifold (or surface in lower dimensional space). The data is then mapped onto a lower-dimensional manifold for more eﬃcient processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement such an approach and assess the result. For simplicity, we will assume that both training and test data are available ahead of time, and thus the datasets should be used together for dimensionality reduction, after which you can split off a test set for measuring generalisation error. *Hint: you do not have to reduce number of dimensions to two. You are welcome to use the sklearn library for this question.*\n",
    " \n",
    "<br />\n",
    "\n",
    "<font color='red'>**Write your code in the cell below ...**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import perceptron\n",
    "\n",
    "# n_components refers to the number of dimensions after mapping\n",
    "# n_neighbors is used for graph construction\n",
    "X_iso = manifold.Isomap(n_neighbors=30, n_components=2).fit_transform(X)\n",
    "\n",
    "# split off the test data\n",
    "phi_train, phi_test, y_train, y_test = train_test_split(X_iso, y, \n",
    "                                                    test_size=0.25, \n",
    "                                                    random_state=0)\n",
    "\n",
    "# Create the model\n",
    "clf = perceptron.Perceptron()\n",
    "clf.fit(phi_train, y_train)\n",
    "\n",
    "Error = np.sum(clf.predict(phi_test) != y_test) / float(y_test.shape[0])\n",
    "print(Error)\n",
    "\n",
    "# Create the KMeans model\n",
    "#clf = cluster.KMeans(init='k-means++', n_clusters=10, random_state=42)\n",
    "\n",
    "# Fit the training data to the model\n",
    "#clf.fit(X_train)\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.plot(X_iso[np.where(clf.predict(phi_test)==-1),0], X_iso[np.where(clf.predict(phi_test)==-1),1], \"bo\")\n",
    "ax1.plot(X_iso[np.where(clf.predict(phi_test)==1),0], X_iso[np.where(clf.predict(phi_test)==1),1], \"ro\")\n",
    "ax1.set_title('Predicted Training Labels for spectral')\n",
    "ax2.plot(X_iso[np.where(y_test==-1),0], X_iso[np.where(y_test==-1),1], \"bo\")\n",
    "ax2.plot(X_iso[np.where(y_test==1),0], X_iso[np.where(y_test==1),1], \"ro\")\n",
    "ax2.set_title('Actual Training Labels for Isomap')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split off the test data\n",
    "phi_train, phi_test, y_train, y_test = train_test_split(X_se, y, \n",
    "                                                    test_size=0.25, \n",
    "                                                    random_state=0)\n",
    "\n",
    "# Create the model\n",
    "clf = perceptron.Perceptron()\n",
    "clf.fit(phi_train, y_train)\n",
    "\n",
    "Error = np.sum(clf.predict(phi_test) != y_test) / float(y_test.shape[0])\n",
    "print(Error)\n",
    "\n",
    "# Create the KMeans model\n",
    "#clf = cluster.KMeans(init='k-means++', n_clusters=10, random_state=42)\n",
    "\n",
    "# Fit the training data to the model\n",
    "#clf.fit(X_train)\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.plot(X_se[np.where(clf.predict(phi_test)==-1),0], X_se[np.where(clf.predict(phi_test)==-1),1], \"bo\")\n",
    "ax1.plot(X_se[np.where(clf.predict(phi_test)==1),0], X_se[np.where(clf.predict(phi_test)==1),1], \"ro\")\n",
    "ax1.set_title('Predicted Training Labels for spectral')\n",
    "ax2.plot(X_se[np.where(y_test==-1),0], X_se[np.where(y_test==-1),1], \"bo\")\n",
    "ax2.plot(X_se[np.where(y_test==1),0], X_se[np.where(y_test==1),1], \"ro\")\n",
    "ax2.set_title('Actual Training Labels for spectral')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a few sentences, comment on the merits of the dimensionality reduction based approach compared to linear classification from Section 1.1 and basis expansion from Section 1.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**Write your answer here ...**</font> (as a *markdown* cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the error rate and the plots, we can find the performance of dimensionality reduction approach is pretty good, especially from the plots, which make the results clear and intuitive.\n",
    "\n",
    "The merits over the linear classification is the classification becoming obviously faster, as well as getting high accuracy easily, which is because:\n",
    "1. It reduces the time and storage space required.\n",
    "2. Removal of multi-collinearity improves the performance of the machine learning model.\n",
    "\n",
    "Besides, it becomes easier to visualize the data when reduced to very low dimensions such as 2D or 3D, which could be more intuitive to look at the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Kaggle Competition\n",
    "The final part of the project is a competition, on more challenging digit data sourced from natural scenes. This data is coloured, pixelated or otherwise blurry, and the digits are not perfectly centered. It is often difficult for humans to classify! The dataset is also considerably larger. \n",
    "\n",
    "Please sign up to the [COMP90051 Kaggle competition](https://inclass.kaggle.com/c/comp90051-2017) using your `student.unimelb.edu.au` email address. Then download the file `data.npz` from Kaggle. This is a compressed `numpy` data file containing three ndarray objects:\n",
    " - `train_X` training set, with 4096 input features (greyscale pixel values);\n",
    " - `train_Y` training labels (0-9)\n",
    " - `test_X` test set, with 4096 input features, as per above\n",
    " \n",
    "Each image is 64x64 pixels in size, which has been flattened into a vector of 4096 values. You should load the files using `np.load`, from which you can extract the three elements. You may need to transpose the images for display, as they were flattened in a different order. Each pixel has an intensity value between 0-255. For those using languages other than python, you may need to output these objects in another format, e.g., as a matlab matrix.\n",
    "\n",
    "Your job is to develop a *multiclass* classifier on this dataset. You can use whatever techniques you like, such as the perceptron code from above, or other methods such as *k*NN, logistic regression, neural networks, etc. You may want to compare several methods, or try an ensemble combination of systems. You are free to use any python libraries for this question. Note that some fancy machine learning algorithms can take several hours or days to train (we impose no time limits), so please start early to allow sufficient time. *Note that you may want to sample smaller training sets, if runtime is an issue, however this will degrade your accuracy. Sub-sampling is a sensible strategy when developing your code.*\n",
    "\n",
    "You may also want to do some basic image processing, however, as this is not part of the subject, we would suggest that you focus most of your efforts on the machine learning. For inspiration, please see [Yan Lecun's MNIST page](http://yann.lecun.com/exdb/mnist/), specifically the table of results and the listed papers. Note that your dataset is harder than MNIST, so your mileage may vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "# load the files\n",
    "train_X = np.load('data/train_X.npy')\n",
    "train_y = np.load('data/train_y.npy')\n",
    "test_X = np.load('data/test_X.npy')\n",
    "print(train_X.shape)\n",
    "print(train_X[0].shape)\n",
    "\n",
    "#plt.imshow(train_X[0,:].reshape(64, 64), interpolation=None)\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.imshow(train_X[0].reshape(64,64), cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(222)\n",
    "plt.imshow(train_X[1].reshape(64,64), cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(223)\n",
    "plt.imshow(train_X[2].reshape(64,64), cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(224)\n",
    "plt.imshow(train_X[3].reshape(64,64), cmap=plt.get_cmap('gray'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "# Import `train_test_split`\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# load the files\n",
    "train_X = np.load('data/train_X.npy')\n",
    "train_y = np.load('data/train_y.npy')\n",
    "test_X = np.load('data/test_X.npy')\n",
    "\n",
    "# We can also reduce our memory requirements \n",
    "# by forcing the precision of the pixel values to be 32 bit\n",
    "train_X = train_X.astype('float32')\n",
    "test_X = test_X.astype('float32')\n",
    "\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "train_X = train_X / 255\n",
    "test_X = test_X / 255\n",
    "\n",
    "# one hot encode outputs\n",
    "#train_y = np_utils.to_categorical(train_y)\n",
    "\n",
    "# Split the data into training and test sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(train_X, train_y, test_size=0.25, random_state=42)\n",
    "\n",
    "#----------------------------SVC--------------------------------\n",
    "# Import the `svm` model\n",
    "from sklearn import svm\n",
    "\n",
    "# Create the SVC model \n",
    "svc_model = svm.SVC(gamma=0.001, C=100., kernel='linear')\n",
    "\n",
    "# Fit the data to the SVC model\n",
    "svc_model.fit(X_train, y_train)\n",
    "\n",
    "print(svc_model.score(X_test, y_test))\n",
    "\n",
    "#----------------------SVM candidates----------------------------\n",
    "# Set the parameter candidates\n",
    "parameter_candidates = [\n",
    "  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n",
    "]\n",
    "\n",
    "# Create a classifier with the parameter candidates\n",
    "clf = GridSearchCV(estimator=svm.SVC(), param_grid=parameter_candidates, n_jobs=1)\n",
    "\n",
    "# Train the classifier on training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Print out the results \n",
    "print('Best score for training data:', clf.best_score_)\n",
    "print('Best `C`:',clf.best_estimator_.C)\n",
    "print('Best kernel:',clf.best_estimator_.kernel)\n",
    "print('Best `gamma`:',clf.best_estimator_.gamma)\n",
    "\n",
    "# Apply the classifier to the test data, and view the accuracy score\n",
    "print(clf.score(X_test, y_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------------------------Baseline Model with Multi-Layer Perceptrons------------------------\n",
    "%pylab inline\n",
    "# Import `train_test_split`\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# load the files\n",
    "train_X = np.load('data/train_X.npy')\n",
    "train_y = np.load('data/train_y.npy')\n",
    "test_X = np.load('data/test_X.npy')\n",
    "print(train_X.shape)\n",
    "\n",
    "# We can also reduce our memory requirements \n",
    "# by forcing the precision of the pixel values to be 32 bit\n",
    "train_X = train_X.astype('float32')\n",
    "test_X = test_X.astype('float32')\n",
    "\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "train_X = train_X / 255\n",
    "test_X = test_X / 255\n",
    "\n",
    "# one hot encode outputs\n",
    "train_y = np_utils.to_categorical(train_y)\n",
    "\n",
    "# Split the data into training and test sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(train_X, train_y, test_size=0.25, random_state=42)\n",
    "\n",
    "num_pixels = X_train.shape[1]\n",
    "num_classes = y_train.shape[1]\n",
    "\n",
    "# define baseline model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_pixels, input_dim=num_pixels, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# build the model\n",
    "model = baseline_model()\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=200, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))\n",
    "\n",
    "print('end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------------------------Simple Convolutional Neural Network--------------------------\n",
    "%pylab inline\n",
    "# Import `train_test_split`\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# load the files\n",
    "train_X = np.load('data/train_X.npy')\n",
    "train_y = np.load('data/train_y.npy')\n",
    "test_X = np.load('data/test_X.npy')\n",
    "\n",
    "# reshape to be [samples][pixels][width][height]\n",
    "train_X = train_X.reshape(train_X.shape[0], 1, 64, 64).astype('float32')\n",
    "test_X = test_X.reshape(test_X.shape[0], 1, 64, 64).astype('float32')\n",
    "\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "train_X = train_X / 255\n",
    "test_X = test_X / 255\n",
    "# one hot encode outputs\n",
    "train_y = np_utils.to_categorical(train_y)\n",
    "num_classes = train_y.shape[1]\n",
    "\n",
    "# Split the data into training and test sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(train_X, train_y, test_size=0.25, random_state=42)\n",
    "\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (5, 5), input_shape=(1, 64, 64), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# build the model\n",
    "model = baseline_model()\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=400, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))\n",
    "\n",
    "np.savetxt(\"simple.csv\",model.predict(test_X).argmax(1),delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------Larger Convolutional Neural Network-------------------\n",
    "%pylab inline\n",
    "# Import `train_test_split`\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# load the files\n",
    "train_X = np.load('data/train_X.npy')\n",
    "train_y = np.load('data/train_y.npy')\n",
    "test_X = np.load('data/test_X.npy')\n",
    "\n",
    "# reshape to be [samples][pixels][width][height]\n",
    "train_X = train_X.reshape(train_X.shape[0], 1, 64, 64).astype('float32')\n",
    "test_X = test_X.reshape(test_X.shape[0], 1, 64, 64).astype('float32')\n",
    "\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "train_X = train_X / 255\n",
    "test_X = test_X / 255\n",
    "# one hot encode outputs\n",
    "train_y = np_utils.to_categorical(train_y)\n",
    "num_classes = train_y.shape[1]\n",
    "\n",
    "# Split the data into training and test sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(train_X, train_y, test_size=0.25, random_state=42)\n",
    "\n",
    "# define the larger model\n",
    "def larger_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(30, (5, 5), input_shape=(1, 64, 64), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(15, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# build the model\n",
    "model = larger_model()\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=200)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Large CNN Error: %.2f%%\" % (100-scores[1]*100))\n",
    "\n",
    "np.savetxt(\"large_2.csv\",model.predict(test_X).argmax(1),delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------Visualization of results---------------------------------\n",
    "from sklearn import manifold\n",
    "# Create an isomap and fit the `digits` data to it\n",
    "X_test_reshape = X_test.reshape(X_test.shape[0], 4096)\n",
    "\n",
    "X_iso = manifold.Isomap(n_neighbors=10).fit_transform(X_test_reshape[0:1000])\n",
    "print('prediction begins')\n",
    "# Compute cluster centers and predict cluster index for each sample\n",
    "predicted = model.predict(X_test[0:1000]).argmax(1)\n",
    "print('plot')\n",
    "# Create a plot with subplots in a grid of 1X2\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "# Adjust the layout\n",
    "fig.subplots_adjust(top=0.85)\n",
    "\n",
    "y_test_color = y_test[0:1000].argmax(1)\n",
    "\n",
    "# Add scatterplots to the subplots \n",
    "ax[0].scatter(X_iso[:, 0], X_iso[:, 1], c=predicted)\n",
    "ax[0].set_title('Predicted labels')\n",
    "ax[1].scatter(X_iso[:, 0], X_iso[:, 1], c=y_test_color)\n",
    "ax[1].set_title('Actual Labels')\n",
    "\n",
    "\n",
    "# Add title\n",
    "fig.suptitle('Predicted versus actual labels', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------------------------Increase batch size-----------------------------------\n",
    "# build the model\n",
    "model = larger_model()\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=2000)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Large CNN Error: %.2f%%\" % (100-scores[1]*100))\n",
    "\n",
    "np.savetxt(\"large_3.csv\",model.predict(test_X).argmax(1),delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#----------------------------Increased layer larger CNN-------------------------------\n",
    "%pylab inline\n",
    "# Import `train_test_split`\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# load the files\n",
    "train_X = np.load('data/train_X.npy')\n",
    "train_y = np.load('data/train_y.npy')\n",
    "test_X = np.load('data/test_X.npy')\n",
    "\n",
    "# reshape to be [samples][pixels][width][height]\n",
    "train_X = train_X.reshape(train_X.shape[0], 1, 64, 64).astype('float32')\n",
    "test_X = test_X.reshape(test_X.shape[0], 1, 64, 64).astype('float32')\n",
    "\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "train_X = train_X / 255\n",
    "test_X = test_X / 255\n",
    "# one hot encode outputs\n",
    "train_y = np_utils.to_categorical(train_y)\n",
    "num_classes = train_y.shape[1]\n",
    "\n",
    "# Split the data into training and test sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(train_X, train_y, test_size=0.25, random_state=42)\n",
    "\n",
    "# define the larger model\n",
    "def larger_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (10, 10), input_shape=(1, 64, 64), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(32, (5, 5), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(16, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# build the model\n",
    "model = larger_model()\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=200)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Large CNN Error: %.2f%%\" % (100-scores[1]*100))\n",
    "\n",
    "np.savetxt(\"large_4.csv\",model.predict(test_X).argmax(1),delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Making Submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be setup as a *Kaggle in class* competition, in which you can upload your system predictions on the test set. You should format your predictions as a csv file, with the same number of lines as the test set, and each line comprising two numbers `id, class` where *id* is the instance number (increasing integers starting from 1) and *class* is an integer between 0-9, corresponding to your system prediction. E.g., \n",
    "```\n",
    "Id,Label\n",
    "1,9\n",
    "2,9\n",
    "3,4\n",
    "4,5\n",
    "5,1\n",
    "...```\n",
    "based on the first five predictions of the system being classes `9 9 4 5 1`. See the `sample_submission.csv` for an example file.\n",
    "\n",
    "Kaggle will report your accuracy on a public portion of the test set, and maintain a leaderboard showing the performance of you and your classmates. You will be allowed to upload up to four submissions each day. At the end of the competition, you should nominate your best submission, which will be scored on the private portion of the test set. The accuracy of your system (i.e., proportion of correctly classified examples) on the private test set will be used for grading your approach.\n",
    "\n",
    "**Marks will be assigned as follows**:\n",
    " - position in the class, where all students are ranked and then the ranks are linearly scaled to <br>0 marks (worst in class) - 4 marks (best in class) \n",
    " - absolute performance (4 marks), banded as follows (rounded to nearest integer): \n",
    " <br>below 80% = 0 marks; 80-89% = 1; 90-92% = 2; 93-94% = 3; above 95% = 4 marks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you are required to submit your code with this notebook, submitted to the LMS. Failure to provide your implementation may result in assigning zero marks for the competition part, irrespective of the competition standing. Your implementation should be able to exactly reproduce submitted final Kaggle entry, and match your description below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Method Description\n",
    "Describe your approach, and justify each of the choices made within your approach. You should write a document with no more than 400 words, as a **PDF** file (not *docx* etc) with up to 2 pages of A4 (2 sides). Text must only appear on the first page, while the second page is for *figures and tables only*. Please use a font size of 11pt or higher. Please consider using `pdflatex` for the report, as it's considerably better for this purpose than wysiwyg document editors. You are encouraged to include empirical results, e.g., a table of results, graphs, or other figures to support your argument. *(this will contribute 9 marks; note that we are looking for clear presentation, sound reasoning, good evaluation and error analysis, as well as general ambition of approach.)*"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
